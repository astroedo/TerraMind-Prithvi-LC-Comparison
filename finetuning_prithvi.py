# -*- coding: utf-8 -*-
"""finetuning_prithvi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GwhtmPDWK-_i7avE15s77HXGphLhtZVd

# *Fine-Tuning of Prithvi GFM*

Code to finetune the Geofoundational model Prithvi on 100 images generated by the LUCAS dataset via GEE on Romania

## Build the environment
"""

# Install TerraTorch and dependencies
!pip install terratorch
!pip install gdown tensorboard lightning

import os
import shutil
from pathlib import Path
import numpy as np
from PIL import Image
import torch
import gdown
import lightning.pytorch as pl
import matplotlib.pyplot as plt
from terratorch.datamodules import GenericNonGeoClassificationDataModule
from terratorch.tasks import ClassificationTask
import rasterio
import zipfile
import warnings
warnings.filterwarnings("ignore")

"""## Setting repository and hyperparameters"""

PREPARED_ROOT = Path("./prepared")
TRAIN_DIR = PREPARED_ROOT / "train"
VAL_DIR = PREPARED_ROOT / "val"
TEST_DIR = PREPARED_ROOT / "test"
OUTPUT_PATH = "./output/terramind_romania"

IMG_SIZE = 224
NUM_CLASSES = 10
BATCH_SIZE = 8
LEARNING_RATE = 1e-4
MAX_EPOCHS = 30
SEED = 42

pl.seed_everything(SEED)

"""## Downloading data"""

print("="*60)
print("DOWNLOADING DATASET FROM GOOGLE DRIVE")
print("="*60)

FILE_ID = "1bF7f_qgRIEnNySeHwwQLHZv_cCrqqs_H"
DATASET_ZIP = "data_Romania.zip"

if os.path.isfile(DATASET_ZIP):
    print(f"Removing existing {DATASET_ZIP}...")
    os.remove(DATASET_ZIP)

print(f"Downloading {DATASET_ZIP}...")
url = f"https://drive.google.com/uc?id={FILE_ID}&confirm=t"
gdown.download(url, DATASET_ZIP, quiet=False, fuzzy=True)
print("✅ Download complete!")

# Extract dataset
if not os.path.exists("Data"):
    print("Extracting dataset...")
    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:
        zip_ref.extractall(".")
    print("✅ Extraction complete!")

# Find data root
if os.path.exists("Data"):
    DATA_ROOT = Path("Data")
elif os.path.exists("data_Romania/Data"):
    DATA_ROOT = Path("data_Romania/Data")
else:
    raise ValueError("Cannot find Data folder!")

print(f"✅ DATA_ROOT: {DATA_ROOT}")

"""## Split the dataset"""

if not TRAIN_DIR.exists():
    print("\nCreating train/val/test splits...")
    TRAIN_DIR.mkdir(parents=True, exist_ok=True)
    VAL_DIR.mkdir(parents=True, exist_ok=True)
    TEST_DIR.mkdir(parents=True, exist_ok=True)

    classes = [d for d in os.listdir(DATA_ROOT)
               if os.path.isdir(DATA_ROOT / d) and d.isdigit()]

    print(f"Found classes: {sorted(classes)}")

    for cls in classes:
        cls_path = DATA_ROOT / cls
        images = [f for f in os.listdir(cls_path)
                  if f.endswith((".jpg", ".png", ".tif"))]

        if len(images) == 0:
            print(f"Warning: No images in class {cls}")
            continue

        np.random.seed(SEED)
        np.random.shuffle(images)

        n = len(images)
        train_split = int(0.7 * n)
        val_split = int(0.9 * n)

        print(f"Class {cls}: {n} images (train={train_split}, val={val_split-train_split}, test={n-val_split})")

        for i, img_name in enumerate(images):
            src = cls_path / img_name
            if i < train_split:
                dst_dir = TRAIN_DIR / cls
            elif i < val_split:
                dst_dir = VAL_DIR / cls
            else:
                dst_dir = TEST_DIR / cls

            dst_dir.mkdir(parents=True, exist_ok=True)
            shutil.copy(src, dst_dir)

    print("✅ Dataset split created")
else:
    print("✅ Dataset splits already exist")

"""## COMPUTING DATASET STATISTICS (ALL BANDS)"""

all_pixels = []
num_bands = None

for cls in os.listdir(TRAIN_DIR):
    cls_folder = TRAIN_DIR / cls
    if not cls_folder.is_dir():
        continue

    for img_name in os.listdir(cls_folder):
        img_path = cls_folder / img_name
        if img_path.suffix.lower() != ".tif":
            continue

        try:
            with rasterio.open(img_path) as src:
                arr = src.read()  # shape: (bands, H, W)

                if arr.shape[0] == 0:
                    continue

                if num_bands is None:
                    num_bands = arr.shape[0]
                    print(f"Detected {num_bands} bands in images")

                # Transpose to (H, W, bands)
                arr = np.transpose(arr, (1, 2, 0))
                arr = arr.astype(np.float32)

                # Normalize per band
                for band_idx in range(arr.shape[2]):
                    band_max = arr[:, :, band_idx].max()
                    if band_max > 0:
                        arr[:, :, band_idx] = arr[:, :, band_idx] / band_max

                # Resize each band
                resized_bands = []
                for band_idx in range(arr.shape[2]):
                    band = Image.fromarray((arr[:, :, band_idx] * 255).astype(np.uint8))
                    band = band.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)
                    resized_bands.append(np.array(band) / 255.0)

                arr_resized = np.stack(resized_bands, axis=-1)
                all_pixels.append(arr_resized)

        except Exception as e:
            print(f"Error processing {img_name}: {e}")

if len(all_pixels) == 0:
    raise RuntimeError("No valid images found!")

all_pixels_stacked = np.stack(all_pixels)
means = all_pixels_stacked.mean(axis=(0, 1, 2)).tolist()
stds = all_pixels_stacked.std(axis=(0, 1, 2)).tolist()

# Ensure no zero std (would cause NaN in normalization)
stds = [max(s, 1e-6) for s in stds]

print(f"✅ Means: {means}")
print(f"✅ Stds: {stds}")
print(f"✅ Channels: {len(means)}")

"""## Creating datamodule"""

datamodule = GenericNonGeoClassificationDataModule(
    train_data_root=str(TRAIN_DIR),
    val_data_root=str(VAL_DIR),
    test_data_root=str(TEST_DIR),
    batch_size=BATCH_SIZE,
    num_workers=2,
    num_classes=NUM_CLASSES,
    means=means,
    stds=stds,
)

print("✅ DataModule created")

"""## Creating model"""

num_channels = len(means)
print(f"Using {num_channels} channels")

# Generate band names for all channels
all_band_names = [f'BAND_{i+1}' for i in range(num_channels)]

# FIX: Create CrossEntropyLoss with proper ignore_index
criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)

task = ClassificationTask(
    model_args={
        "backbone": "prithvi_eo_v2_300",
        "backbone_pretrained": True,
        "backbone_bands": all_band_names,  # All 7 bands
        "backbone_num_frames": 1,
        "decoder": "IdentityDecoder",
        "head_dropout": 0.1,
        "num_classes": NUM_CLASSES
    },
    model_factory="EncoderDecoderFactory",
    loss=criterion,
    lr=LEARNING_RATE,
    aux_loss={},
    optimizer="AdamW",
    optimizer_hparams={"weight_decay": 0.05},
    class_names=[str(i+1) for i in range(NUM_CLASSES)],
)

print("✅ Task created")

"""## Configuring trainer"""

checkpoint_dir = os.path.join(OUTPUT_PATH, "checkpoints")
os.makedirs(checkpoint_dir, exist_ok=True)

checkpoint_callback = pl.callbacks.ModelCheckpoint(
    dirpath=checkpoint_dir,
    monitor="val/Accuracy",
    mode="max",
    filename="best-acc-{epoch:02d}-{val/Accuracy:.3f}",
    save_top_k=3,
    save_weights_only=True,
)

trainer = pl.Trainer(
    accelerator="auto",
    devices=1,
    max_epochs=MAX_EPOCHS,
    precision="16-mixed",
    callbacks=[
        checkpoint_callback,
        pl.callbacks.RichProgressBar(),
        pl.callbacks.LearningRateMonitor(logging_interval="epoch"),
    ],
    default_root_dir=OUTPUT_PATH,
    log_every_n_steps=10,
    num_sanity_val_steps=2,
    gradient_clip_val=1.0,
)

print("✅ Trainer configured")

"""## Start training"""

try:
    trainer.fit(task, datamodule=datamodule)
    print(f"\n✅ Training complete!")
    print(f"Best model: {checkpoint_callback.best_model_path}")

    # ====== TEST ======
    print("\n" + "="*60)
    print("TESTING MODEL")
    print("="*60)

    if checkpoint_callback.best_model_path:
        trainer.test(task, datamodule=datamodule, ckpt_path=checkpoint_callback.best_model_path)
    else:
        print("⚠️ No checkpoint saved, testing with current model")
        trainer.test(task, datamodule=datamodule)

except Exception as e:
    print(f"\n❌ Training failed with error: {e}")
    import traceback
    traceback.print_exc()

"""## Visualization section"""

try:
    # Load best model
    best_model = ClassificationTask.load_from_checkpoint(
        checkpoint_callback.best_model_path,
        model_factory="EncoderDecoderFactory",
    )
    best_model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    best_model = best_model.to(device)

    # Get test batch
    test_loader = datamodule.test_dataloader()
    batch = next(iter(test_loader))

    # Extract images and labels correctly
    # The batch structure from TerraTorch is: {"image": tensor, "label": tensor}
    images = batch["image"]
    labels = batch["label"]

    # Move to device
    images = images.to(device)

    # For Prithvi, we need to wrap the tensor in a dict with the modality name
    # TerraTorch expects: {"optical": tensor} format for the model
    images_dict = {"optical": images}

    # Get predictions
    with torch.no_grad():
        outputs = best_model(images_dict)
        preds = torch.argmax(outputs.output, dim=1).cpu().numpy()

    # Visualize
    fig, axes = plt.subplots(5, 3, figsize=(12, 15))
    class_names = [str(i+1) for i in range(NUM_CLASSES)]

    num_samples = min(5, len(labels))
    for i in range(num_samples):
        # Get image (use first 3 bands for RGB visualization)
        img = images[i].cpu().permute(1, 2, 0).numpy()
        img = (img - img.min()) / (img.max() - img.min() + 1e-8)

        if img.shape[2] > 3:
            img = img[:, :, :3]

        axes[i, 0].imshow(img)
        axes[i, 0].set_title(f"Sample {i+1}")
        axes[i, 0].axis('off')

        axes[i, 1].text(0.5, 0.5, f"True: Class {class_names[labels[i]]}",
                        ha='center', va='center', fontsize=14)
        axes[i, 1].axis('off')

        is_correct = preds[i] == labels[i]
        axes[i, 2].text(0.5, 0.5, f"Pred: Class {class_names[preds[i]]}",
                        ha='center', va='center', fontsize=14,
                        color='green' if is_correct else 'red')
        axes[i, 2].axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_PATH, 'predictions.png'), dpi=150, bbox_inches='tight')
    plt.show()

    print("✅ Visualization complete!")

except Exception as e:
    print(f"⚠️ Visualization failed: {e}")
    import traceback
    traceback.print_exc()

print("\n✅ ALL DONE!")